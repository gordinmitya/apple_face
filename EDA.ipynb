{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "import data.data_iterators as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "IMAGES_DIR = '/media/pupa/DataStorage/datasets'\n",
    "ANNOTATIONS_DIR = '/home/pupa/dev/apple_face/annotations'\n",
    "\n",
    "_300w_train, _300w_test  = (\n",
    "    list(data.get_300W_train(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    "    list(data.get_300W_test(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    ")\n",
    "celeba_train, celeba_test = (\n",
    "    list(data.get_celeba_train(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    "    list(data.get_celeba_test(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    ")\n",
    "ffhq_train, ffhq_test = (\n",
    "    list(data.get_ffhq_train(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    "    list(data.get_ffhq_test(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    ")\n",
    "wflw_train, wflw_test = (\n",
    "    list(data.get_WFLW_train(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    "    list(data.get_WFLW_test(IMAGES_DIR, ANNOTATIONS_DIR)),\n",
    ")\n",
    "all_train = [*_300w_train, *celeba_train, *ffhq_train, *wflw_train]\n",
    "all_test = [*_300w_test, *celeba_test, *ffhq_test, *wflw_test]\n",
    "all_data = [*all_train, *all_test]\n",
    "\n",
    "print(f'300W Train: {len(_300w_train)} faces {sum(len(x.faces) for x in _300w_train)}; ' \\\n",
    "        + f'Test: {len(_300w_test)} faces {sum(len(x.faces) for x in _300w_test)}')\n",
    "print(f'CelebA Train: {len(celeba_train)} faces {sum(len(x.faces) for x in celeba_train)}; ' \\\n",
    "        + f'Test: {len(celeba_test)} faces {sum(len(x.faces) for x in celeba_test)}')\n",
    "print(f'FFHQ Train: {len(ffhq_train)} faces {sum(len(x.faces) for x in ffhq_train)}; ' \\\n",
    "        + f'Test: {len(ffhq_test)} faces {sum(len(x.faces) for x in ffhq_test)}')\n",
    "print(f'WFLW Train: {len(wflw_train)} faces {sum(len(x.faces) for x in wflw_train)}; ' \\\n",
    "        + f'Test: {len(wflw_test)} faces {sum(len(x.faces) for x in wflw_test)}')\n",
    "\n",
    "# please note that the number of images in test set is not the same as in original repo\n",
    "# eg for wflw it supposed to be 2500 records in test split, however:\n",
    "# 1. if image contains several faces, it is counted as several records\n",
    "# 2. apple engine may find less/more images than was manually annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_crop(img, landmarks):\n",
    "    for landmark in landmarks:\n",
    "        x, y = int(landmark[0]), int(landmark[1])\n",
    "        img = cv2.circle(img, (x, y), 3, (0, 0, 255), -1)\n",
    "    return img\n",
    "\n",
    "def draw_bboxes(img, bbox):\n",
    "    left, top, right, bottom = bbox['left'], bbox['top'], bbox['right'], bbox['bottom']\n",
    "    img = cv2.rectangle(img, (left, top), (right, bottom), (255, 0, 0), 1)\n",
    "    img = cv2.circle(img, (left, top), 7, (0, 0, 0), -1)\n",
    "    img = cv2.circle(img, (right, bottom), 7, (255, 255, 255), -1)\n",
    "    return img\n",
    "\n",
    "samples_data: list[data.ImageWithFaces] = [\n",
    "    _300w_train[0],\n",
    "    celeba_train[0],\n",
    "    ffhq_train[0],\n",
    "    wflw_train[0],\n",
    "]\n",
    "samples_images = []\n",
    "for s in samples_data:\n",
    "    img = cv2.cvtColor(cv2.imread(s.image_path), cv2.COLOR_BGR2RGB)\n",
    "    print(s.image_path)\n",
    "    for face in s.faces:\n",
    "        bbox, landmarks = face.bbox, face.landmarks\n",
    "        print(bbox)\n",
    "        img = draw_bboxes(img, bbox)\n",
    "        img = draw_landmarks_crop(img, landmarks)\n",
    "    samples_images.append(img)\n",
    "min_height = min([img.shape[0] for img in samples_images])\n",
    "samples_images = [cv2.resize(img, (int(img.shape[1] * min_height / img.shape[0]), min_height)) for img in samples_images]\n",
    "samples_images = np.concatenate(samples_images, axis=1)\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(samples_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_count = 0\n",
    "face_per_img = {}\n",
    "image_sizes = []\n",
    "face_sizes = []\n",
    "for item in all_data:\n",
    "    images_count += 1\n",
    "    image_sizes.append((item.image_size[0], item.image_size[1]))\n",
    "    face_per_img[len(item.faces)] = face_per_img.get(len(item.faces), 0) + 1\n",
    "    for face in item.faces:\n",
    "        bbox = face.bbox\n",
    "        width = bbox['right'] - bbox['left']\n",
    "        height = bbox['bottom'] - bbox['top']\n",
    "        assert width > 0 and height > 0, item.image_path\n",
    "        face_sizes.append((width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [key for key, val in face_per_img.items() for _ in range(val)]\n",
    "plt.hist(mylist, bins=64, log=True, )\n",
    "plt.title('Face count per image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = np.array(image_sizes)\n",
    "plt.hist2d(image_sizes[:, 0], image_sizes[:, 1], bins=64, norm=matplotlib.colors.LogNorm())\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "plt.title('Image sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_sizes = np.array(face_sizes)\n",
    "plt.hist2d(face_sizes[:, 0], face_sizes[:, 1], bins=48, norm=matplotlib.colors.LogNorm())\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "plt.title('Face sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae6eeddc573c4c71d980054ec3dc9f5d051f5329c83e779a1fbcc2c12ae1a8a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
